# Collection of Content made for the CAIML Meetup #29 Talk by Emil Bohleber

## Advanced Improvement Strategies for Your RAG Pipeline

In recent times, as Large Language Models (LLMs) have gained popularity in the tech world, there's been a big push to find ways to make these models even better by feeding them data that's specific to certain areas or topics. One of the standout methods for doing this is called Retrieval Augmented Generation, or RAG for short. But, like anything else, RAG comes with its own set of issues, such as Vector similarity and vector space density issues, or sparse retrieval challenges. In my talk, I'll dive into some strategies that have shown promise in tackling these issues. We'll go through examples to see these strategies in action and we'll take a look at some code to understand how these solutions work in practice, and what kind of improvements we might expect from them.

## Slides

Slides are located in the /docs folder. You can view them [here](https://brickmakersgmbh.github.io/CAIML_RAG).

## Code

The code is located in the /code folder. We're using Jupiter notebooks to run the code. You can run the code by installing the requirements and running the notebooks.

## Sources

- [RAG: Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)
- [Precise Zero-Shot Dense Retrieval without Relevance Labels](<https://arxiv.org/pdf/2212.10496.pdf>)
- [LangChain Documentation](https://python.langchain.com/docs/get_started)
- [A Practical Approach to Retrieval Augmented Generation Systems](https://mallahyari.github.io/rag-ebook/04_advanced_rag.html)
- [How to improve RAG peformance â€” Advanced RAG Patterns](https://cloudatlas.me/how-to-improve-rag-peformance-advanced-rag-patterns-part2-0c84e2df66e6)
- [Text Embeddings: Comprehensive Guide](https://towardsdatascience.com/text-embeddings-comprehensive-guide-afd97fce8fb5)
